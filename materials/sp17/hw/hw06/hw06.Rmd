---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region {"deletable": true, "editable": true} -->
# Homework 6: Confidence Intervals and Sample Size

This assignment is due Thursday, April 6 at 11:59PM. You will receive an early submission bonus point if you turn it in by Wednesday, April 5 at 11:59PM. Directly sharing answers is not okay, but discussing problems with course staff or with other students is encouraged.

Reading:
- [Chapter 11](https://www.inferentialthinking.com/chapters/11/estimation.html): confidence intervals
- [Chapter 11.3](https://www.inferentialthinking.com/chapters/11/3/confidence-intervals.html): interpreting confidence intervals
- [Chapter 12](https://www.inferentialthinking.com/chapters/12/why-the-mean-matters.html): center/spread
- [Chapter 12.5](https://www.inferentialthinking.com/chapters/12/5/variability-of-the-sample-mean.html): choosing a sample size
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
Run the cell below to prepare the notebook and the tests. **Passing the automatic tests does not guarantee full credit on any question.** The tests are provided to help catch some common errors, but it is *your* responsibility to answer the questions correctly.
<!-- #endregion -->

```{python}
# Don't change this cell; just run it. 

import numpy as np
from datascience import *

# %matplotlib inline
import matplotlib.pyplot as plots
plots.style.use('fivethirtyeight')

from client.api.notebook import Notebook
ok = Notebook('hw06.ok')
_ = ok.auth(inline=True)
```

Once you're finished, select "Save and Checkpoint" in the File menu and then execute the `submit` cell below. The result will contain a link that you can use to check that your assignment has been submitted successfully. If you submit more than once before the deadline, we will only grade your final submission.

```{python}
_ = ok.submit()
```

<!-- #region {"deletable": true, "editable": true} -->
## 1. Plot the Vote

<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
Four candidates are running for President of Dataland. A polling company surveys 1000 people selected uniformly at random from among voters in Dataland, and it asks each one who they are planning on voting for. After compiling the results, the polling company releases the following proportions from their sample:

|Candidate  | Proportion|
|:------------:|:------------:|
|Candidate C | 0.47 |
|Candidate T | 0.38 |
|Candidate J | 0.08 |
|Candidate S | 0.03 |
|Undecided   | 0.04 |

These proportions represent a uniform random sample of the population of Dataland. We will attempt to estimate the corresponding *population parameters* - the proportions of each kind of voter in the entire population.  We will use confidence intervals to compute a range of values that reflects the uncertainty of our estimate.

The table `voters` contains the results of the survey. Candidates are represented by their initials. Undecided voters are denoted by `U`.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
votes = Table().with_column('vote', np.array(['C']*470 + ['T']*380 + ['J']*80 + ['S']*30 + ['U']*40))
num_votes = votes.num_rows
votes.sample()
```

<!-- #region {"deletable": true, "editable": true} -->
Below, we have give you code that will use bootstrapped samples to compute estimates of the true proportion of voters who are planning on voting for **Candidate C**.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def proportions_in_resamples():
    statistics = make_array()
    for i in np.arange(5000):
        bootstrap = votes.sample()
        sample_statistic = np.count_nonzero(bootstrap.column('vote') == 'C')/num_votes
        statistics = np.append(statistics, sample_statistic)
    return statistics

sampled_proportions = proportions_in_resamples()
Table().with_column('Estimated Proportion', sampled_proportions).hist(bins=np.arange(0.2,0.6,0.01))
```

<!-- #region {"deletable": true, "editable": true} -->
**Question 1.** Using the array `sampled_proportions`, compute an approximate 95% confidence interval for the true proportions of voters planning on voting for candidate C.  (Compute the lower and upper ends of the interval, named `lower_bound` and `upper_bound`, respectively.)
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
c_lower_bound = ...
c_upper_bound = ...
print("Bootstrapped 95% confidence interval for the proportion of C voters in the population: [{:f}, {:f}]".format(c_lower_bound, c_upper_bound))
```

```{python}
_ = ok.grade('q1_1')
```

<!-- #region {"deletable": true, "editable": true} -->
**Question 2.** The survey results seem to indicate that Candidate C is beating Candidate T among voters. We would like to use CI's to determine a range of likely values for her true *lead*. Candidate C's lead over Candidate T is:

$$\text{Candidate C's proportion of the vote} - \text{Candidate T's proportion of the vote}.$$

Using the function `proportions_in_resamples` above as a model, use the bootstrap to compute an approximate distribution for Candidate C's lead over Candidate T. Plot a histogram of the the resulting samples.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE, for_assignment_type=student, manual_problem_id=election_2}
bins = bins=np.arange(-0.2,0.2,0.01)

def leads_in_resamples():
    statistics = make_array()
    ...
    return statistics

sampled_leads = leads_in_resamples()
...
```

```{python deletable=TRUE, editable=TRUE}
diff_lower_bound = ...
diff_upper_bound = ...
print("Bootstrapped 95% confidence interval for Candidate C's true lead over Candidate T: [{:f}, {:f}]".format(diff_lower_bound, diff_upper_bound))
```

```{python}
_ = ok.grade('q1_3')
```

<!-- #region {"deletable": true, "editable": true} -->
## 2. Interpreting Confidence Intervals

<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
The staff computed the following 95% confidence interval for the proportion of Candidate C voters: 

$$[.439, .5]$$

(Your answer might have been different; that doesn't mean it was wrong.)
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 1
Can we say that 95% of the population lies in the range $[.439, .5]$? Explain your answer. 
<!-- #endregion -->

<!-- #region {"manual_problem_id": "interpreting_1"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 2
Can we say that there is a 95% probability that the interval [.439, .5] contains the true proportion of the population who is voting for Candidate C? Explain your answer.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_2"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 3
Suppose we produced 10,000 new samples (each one a uniform random sample of 1,000 voters) and created a 95% confidence interval from each one. Roughly how many of those 10,000 intervals do you expect will actually contain the true proportion of the population?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_3"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
**Question 4**

The staff also created 80%, 90%, and 99% confidence intervals from one sample, but we forgot to label which confidence interval represented which percentages! Match the interval to the percent of confidence the interval represents. (Write the percentage after each interval below.) **Then**, explain your thought process.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_4"} -->
**Answers:**

$[.444,.495]$:

$[.45,.49]$:

$[.43,.511]$:
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 5

Recall the second bootstrap confidence interval you created, estimating Candidate C's lead over Candidate T. Among voters in the sample, her lead was .09. The staff's 95% confidence interval for her true lead (in the population of all voters) was

$$[.032,.15].$$

Suppose we are interested in testing a simple yes-or-no question:

> "Are the candidates tied?"

Our null hypothesis is that the proportions are equal, or, equivalently, that Candidate C's lead is exactly 0. Our alternative hypothesis is that her lead is not equal to 0.  In the questions below, don't compute any confidence interval yourself - use only the staff's 95% confidence interval.


**Question:** Say we use a 5% P-value cutoff.  Do we reject the null, fail to reject the null, or are we unable to tell using our confidence interval?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_5"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 6
What if, instead, we use a P-value cutoff of 1%? Do we reject the null, fail to reject the null, or are we unable to tell using our confidence interval?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_6"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
#### Question 7
What if we use a P-value cutoff of 10%? Do we reject, fail to reject, or are we unable to tell using our confidence interval?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "interpreting_7"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
## 3. Grouped Means

<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
Suppose you'd like to know about the ages of the people in a small town.  The local government collects this data about everyone in the town, but to ensure that you don't see any individual's age, it only makes public the number of people of each age.  (This could have been done by calling `group` on the original data table.)  So the first few rows of the dataset look something like this:
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
ages =  Table().with_columns('age', [0, 1, 2, 3, 5, 6], 'count', [2, 5, 1, 4, 10, 1])
ages
```

<!-- #region {"deletable": true, "editable": true} -->
That means there were 2 people age 0, 5 people age 1, etc. Nobody is age 4.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 1
After you get the data, you first want to compute the mean age of the people in the town.

Write a function called `grouped_mean`.  It should take as its argument a table like the one above, except that the columns might have different names.  It should return the mean of the numbers in the dataset, assuming the first column contains the numbers themselves and the second column contains the count of each number, as in the example.

*Remember:* Even if you don't know the column name for the first column, you can access it by saying `tbl.column(0)`.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def grouped_mean(t):
    assert t.num_columns == 2, 'Expected a 2-column table t'
    ...
```

```{python deletable=TRUE, editable=TRUE}
_ = ok.grade('q3_1')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 2
Next, you want to summarize how spread out the ages are, so you decide to compute their standard deviation.

Write a function called `grouped_std`.  It should take as its argument a table like the one above, except that the columns might have different names.  It should return the standard deviation of the numbers in the dataset, assuming the first column contains the numbers and the second column contains the count of each number, as in the example.

*Hint:* You can think of the standard deviation as the square root of the mean of a dataset that's a transformed version of the original dataset.  The numbers in the transformed dataset are the squared deviations from the mean.  You've already written a function that computes means of grouped numbers, so that should be useful.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def grouped_std(t):
    mean = grouped_mean(t)
    ...
```

```{python deletable=TRUE, editable=TRUE}
_ = ok.grade('q3_2')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
Maybe you aren't sure whether your code for the previous question is correct.  Testing your own code on simple cases is an important skill.  Let's practice that.

The built-in NumPy function `np.std` computes the standard deviation of an array of numbers.  It doesn't work for grouped data, so you couldn't have just used it in your answer to question 2!  But we can use it to check `grouped_std` by manually un-grouping some small datasets (duplicating each number once for each count, and putting the duplicated numbers into an array) and calling `np.std` on the result.

|age|count|
|-|-|
|10|1|
|15|2|

$$\longleftrightarrow$$

$$\verb|make_array(10, 15, 15)|$$
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 3
For the two tables in the following **two** cells, create an array representing the original (un-grouped) dataset it came from, and then use it to verify that `grouped_std` computes the right answer on that table.  We've done most of the first one for you.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE, for_assignment_type=student}
example_0 = Table().with_columns(
    "age", make_array(10, 15),
    "count", make_array(1, 2))
grouped_std_0 = grouped_std(example_0)
example_0_ungrouped = make_array(10, 15, 15)
# The standard deviation of example_0_ungrouped, according to NumPy:
numpy_std_0 = ...
print("NumPy answer:\t", numpy_std_0, "\nyour answer:\t", grouped_std_0)
```

```{python deletable=TRUE, editable=TRUE, for_assignment_type=student}
example_1 = Table().with_columns(
    "age", make_array(10, 15, 20, 25),
    "count", make_array(1, 2, 3, 0))
# Fill in the rest of the test, as above, so that the last line
# prints out the results of the test.
...
print("NumPy answer:\t", numpy_std_1, "\nyour answer:\t", grouped_std_1)
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
If your results are different, that means there's an error in your `grouped_std` function (or your `grouped_mean` function).  Go back and fix it!  Each time you make a change, you can rerun the tests you've written to see if you've gotten it right.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
## 4. Testing the Central Limit Theorem

<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
The Central Limit Theorem tells us that the probability distribution of the sum or average of a large random sample drawn with replacement will be roughly normal, *regardless of the distribution of the population from which the sample is drawn*.

That's a pretty big claim, but the theorem doesn't stop there. It further states that the standard deviation of this normal distribution is given by $$\frac{\texttt{sd of the original distribution}}{\sqrt{\texttt{sample size}}}$$ In other words, suppose we start with *any distribution* that has standard deviation $x$, take a sample of size $n$ (where $n$ is a large number) from that distribution with replacement, and compute the mean of that sample. If we repeat this procedure many times, then those sample means will have a normal distribution with standard deviation $\frac{x}{\sqrt{n}}$.

That's an even bigger claim than the first one! The proof of the theorem is beyond the scope of this class, but in this exercise, we will be exploring some data to see the CLT in action.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
**Question 1.** The CLT only applies when sample sizes are "sufficiently large." This isn't a very precise statement. Is 10 large?  How about 50?  The truth is that it depends both on the original population distribution and just how "normal" you want the result to look. Let's use a simulation to get a feel for how the distribution of the sample mean changes as sample size goes up.

Consider a coin flip. If we say `Heads` is $1$ and `Tails` is $0$, then there's a 50% chance of getting a 1 and a 50% chance of getting a 0, which is definitely not a normal distribution.  The average of several coin tosses is equal to the proportion of heads in those coin tosses, so the CLT should apply if we compute the sample proportion of heads many times.

Write a function called `simulate_sample_n` that takes in a sample size $n$. It should return an array that contains 5000 sample proportions of heads, each from $n$ coin flips.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def sample_size_n(n):
    coin = make_array(0, 1)
    sample_proportions = make_array()
    for i in np.arange(5000):
        # An array of the results of n coin flips (0s and 1s):
        flips = ...
        sample_proportion = ...
        sample_proportions = ...
    return sample_proportions


sample_size_n(5)
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
The code below will use the function you just defined to plot the empirical distribution of the sample mean for several different sample sizes. The x- and y-scales are kept the same to facilitate comparisons.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
bins = np.arange(-0.01,1.05,0.02)

for sample_size in make_array(2, 5, 10, 20, 50, 100, 200, 400):
    Table().with_column('Sample Size: {}'.format(sample_size), sample_size_n(sample_size)).hist(bins=bins)
    plots.ylim(0, 30)
```

<!-- #region {"deletable": true, "editable": true} -->
You can see that even the means of samples of 10 items follow a roughly bell-shaped distribution.  A sample of 50 items looks quite bell-shaped.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "clt_2"} -->
<div class="hide">\pagebreak</div>
**Question 2:** In the plot for a sample size of 10, why are the bars spaced at intervals of .1, with gaps in between?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "clt_2"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
Now we will test the second claim of the CLT: That the SD of the sample mean is the SD of the original distribution, divided by the square root of the sample size.

We have imported the flight delay data and computed its standard deviation for you.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
united = Table.read_table('united_summer2015.csv')
united_std = np.std(united.column('Delay'))
united_std
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
**Question 3:** Write a function called `predict_sd`.  It takes a sample size `n` (a number) as its argument.  It returns the predicted standard deviation of the mean for samples of size `n` from the flight delays.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def predict_sd(n):
    ...

predict_sd(10)
```

```{python deletable=TRUE, editable=TRUE}
_ = ok.grade('q4_3')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
**Question 4:** Write a function called `empirical_sd` that takes a sample size `n` as its argument. The function should simulate 500 samples of size `n` from the flight delays dataset, and it should return the standard deviation of the **means of those 500 samples**.

*Hint:* This function will be similar to the `sample_size_n` function you wrote earlier.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
def empirical_sd(n):
    sample_means = make_array()
    for i in np.arange(500):
        sample = ...
        sample_mean = ...
        sample_means = ...
    return np.std(sample_means)

empirical_sd(10)
```

```{python deletable=TRUE, editable=TRUE}
_ = ok.grade('q4_4')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
The cell below will plot the predicted and empirical SDs for the delay data for various sample sizes.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
sd_table = Table().with_column('Sample Size', np.arange(1,101))
predicted = sd_table.apply(predict_sd, 'Sample Size')
empirical = sd_table.apply(empirical_sd, 'Sample Size')
sd_table = sd_table.with_columns('Predicted SD', predicted, 'Empirical SD', empirical)
sd_table.scatter('Sample Size')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
**Question 5:** The empirical SDs are very close to the predicted SDs, but they're not exactly the same.  Why?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "clt_5"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
## 5. Polling and the Normal Distribution

<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
Michelle is a statistical consultant, and she works for a group that supports Proposition 68 (which would mandate labeling of all horizontal or vertical axes), called Yes on 68.  They want to know how many Californians will vote for the proposition.

Michelle polls a uniform random sample of all California voters, and she finds that 210 of the 400 sampled voters will vote in favor of the proposition.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
sample = Table().with_columns(
    "Vote",  make_array("Yes", "No"),
    "Count", make_array(210,   190))
sample_size = sum(sample.column("Count"))
sample_proportions = sample.with_column(
    "Proportion", sample.column("Count") / sample_size)
sample_proportions
```

<!-- #region {"deletable": true, "editable": true} -->
She uses 10,000 bootstrap resamples to compute a confidence interval for the proportion of all California voters who will vote Yes.  Run the next cell to see the empirical distribution of Yes proportions in the 10,000 resamples.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
resample_yes_proportions = make_array()
for i in np.arange(10000):
    resample = proportions_from_distribution(sample_proportions, "Proportion", sample_size)
    resample_yes_proportions = np.append(resample_yes_proportions, resample.column("Random Sample").item(0))
Table().with_column("Resample Yes proportion", resample_yes_proportions).hist(bins=np.arange(.2, .8, .01))
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 1
Explain how the Central Limit Theorem applies to one of the distributions in this story.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "polling_1"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
In a population whose members are 0 and 1, there is a simple formula for the standard deviation of that population:

$$\texttt{standard deviation} = \sqrt{(\text{proportion of 0s}) \times (\text{proportion of 1s})}$$

(Figuring out this formula, starting from the definition of the standard deviation, is an fun exercise for those who enjoy algebra.)
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 2
**Without accessing the data in `resample_yes_proportions` in any way**, and instead using only the Central Limit Theorem and the numbers of Yes and No voters in our sample of 400, compute a number `approximate_sd` that's the predicted standard deviation of the array `resample_yes_proportions` according to the central limit theorem. Since you don't know the true proportions of 0s and 1s in the population, use the proportions in the sample instead (since they're probably similar).
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
# The standard deviation of the elements of the sample is
# sqrt((210/400) * (1 - 210/400)).  The Central Limit
# Theorem says that the standard deviation of the mean
# of 400 elements sampled from this dataset is the
# standard deviation of the dataset divided by the
# square root of 400.
approximate_sd = ...
approximate_sd
```

```{python}
_ = ok.grade('q5_2')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 3
Compute the standard deviation of the array `resample_yes_proportions` to verify that your answer to question 2 is approximately right.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
exact_sd = ...
exact_sd
```

```{python}
_ = ok.grade('q5_3')
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 4
**Still without accessing `resample_yes_proportions` in any way**, compute an approximate 95% confidence interval for the proportion of Yes voters in California.  The cell below draws your interval as a red bar below the histogram of `resample_yes_proportions`; use that to verify that your answer looks right.
<!-- #endregion -->

```{python deletable=TRUE, editable=TRUE}
lower_limit = ...
upper_limit = ...
print('lower:', lower_limit, 'upper:', upper_limit)
```

```{python}
_ = ok.grade('q5_4')
```

```{python deletable=TRUE, editable=TRUE}
# Run this cell to plot your confidence interval.
Table().with_column("Resample Yes proportion", resample_yes_proportions).hist(bins=np.arange(.2, .8, .01))
plots.plot(make_array(lower_limit, upper_limit), make_array(0, 0), c='r', lw=10);
```

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
Your confidence interval should overlap the number 0.5.  That means we can't be very sure whether Proposition 68 is winning, even though the sample Yes proportion is a bit above 0.5.

The Yes on 68 campaign really needs to know whether they're winning.  It's impossible to be absolutely sure without polling the whole population, but they'd be okay if the standard deviation of the sample mean were only 0.005.  They ask Michelle to run a new poll with a sample size that's large enough to achieve that.  (Polling is expensive, so the sample also shouldn't be bigger than necessary.)

Michelle consults Chapter 12 of your textbook.  Instead of making the conservative assumption that the population standard deviation is 0.5 (coding Yes voters as 1 and No voters as 0), she decides to assume that it's equal to the standard deviation of the sample,

$$\sqrt{(\text{Yes proportion in the sample}) \times (\text{No proportion in the sample})}.$$

Under that assumption, Michelle decides that a sample of 9,975 would suffice.
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true} -->
<div class="hide">\pagebreak</div>
#### Question 5
How did Michelle arrive at that answer?
<!-- #endregion -->

<!-- #region {"deletable": true, "editable": true, "manual_problem_id": "polling_5"} -->
*Write your answer here, replacing this text.*
<!-- #endregion -->
